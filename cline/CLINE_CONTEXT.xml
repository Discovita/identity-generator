<cline-context>
    <rules>
        <!-- These rules are common to all my coding projects. -->
        <rule>
            Never start an import with `from src.`. src is not a module. Also never
            use relative imports beyond a single `.` for same package. We use
            a setup.py file to implement this standard. We also indicate in a vscode
            settings file to open all terminals in a venv.
        </rule>
        <rule>
            You're an experienced senior python engineer. You know how to set up
            a python project correctly using best practices. You never use hacks like setting
            environment variables to get scripts to run. You're an expert at how python works
            under the hood and how python scripts are supposed to be run. Moreover, if you see
            a project is not running as you expect, rather than hacking your way toward a solution,
            you criticize the project and fix it to use a correct directory and file structure
            in accordance with best practices for modern python development.
        </rule>
        <rule>Do not use `ls -R`, there are too many files and it will print too much.</rule>
        <rule>Use `pyright src` to check compiler errors and `pytest` to run unit tests after every
            major
            change.
            Also run `pyright test` for the test code. Use strong typing aggressively. Major
            service functions that are expected to be called by other files should define a request
            and response object data structure.</rule>
        <rule> DO NOT use try-except and do not write None-checks. Strong-type everything and use
            Optional[Foo] if it really is an expected case that the Foo can be present or absent.
            Use assertions to coerce to Foo or throw if Foo is not optional according to "business
            logic". A common example of this is XML parsing errors from LLM responses. The issue is
            almost never that the LLM returned a bad response but that the prompt is misaligned with
            the code. In such cases, we need to fail fast (as always) and fix the prompt. <rule>Don't
            let files exceed 100 lines.
                Break them into modules when they reach this point.</rule>
        <rule>Update the
            `reference` section
                of this XML doc with `note`s.</rule>
        </rule>
        <rule>Keep source code files under 100 lines so they are AI assistant-friendly. When you
            think a file
            needs to grow larger, split it and make it more modular.
        </rule>
        <note>
            Frontend environment variables are configured in the top-level .env file and accessed
            in React components using process.env.REACT_APP_* (e.g.
            process.env.REACT_APP_API_BASE_URL).
            Variables must be prefixed with REACT_APP_ to be accessible in the React application.
        </note>
        <rule>Inline comments are almost always bad and a code smell. Good code should
            explain itself.
        </rule>
        <rule>No try-excepts. Fail fast everywhere possible.</rule>
        <rule>Primary git branch is `master`.</rule>
        <rule>NEVER touch css unless I've EXPLICITLY instructed you to work on
            app styling.</rule>
        <rule>When working with CSS, use variables for shared values across components,
            particularly for colors. Avoid haphazard use of hard-coded values.
        </rule>
        <rule>
          For simple file operations by command line, please batch together
          multiple mkdir / mv commands into a single mutli-line bash command
          so as to avoid wasting time and AI API calls.
        </rule>
    </rules>

    <long-term-memory>
        <!-- -->
        <note>
            I'm building a simple API and UI to automate a workflow
            for doing an image face-swap using an existing API.
        </note>
        <note>
            Start app:
            ```
        <![CDATA[
            cd backend && uvicorn discovita.app:app --reload  # Terminal 1
            cd frontend && npm start                          # Terminal 2
            ]]>
            ```
        </note>
        <note>
            src/scripts/dalle/darth_vader_example.py demonstrates how to generate
            an image and modify the augmented prompts to edit an image by re-prompting
            using the OpenAI client. This will be necessary when we are implementing
            the parts of the project where the user is providing feedback on images.
        </note>
        <note>
            Project uses Pydantic v2. V1 functions like dict() are not allowed.
            Use model_dump() instead of dict() for serialization.
        </note>
        <note>
            TypeScript types must be regenerated when API signatures change:
            1. After modifying Pydantic models in backend/src/discovita/service/coach/models.py
            2. Run the generate-types script: `npm run generate-types` from frontend/apps/coach
            directory
            3. Include the updated apiTypes.ts in your PR
            This ensures frontend type safety is maintained with backend changes.
        </note>
        <note>
        How the coach should work:

The way this works should be quite simple.  

CoachState should have `current_state` which is a `CoachingState` enum value (this is defined in backend/src/discovita/service/coach/models/state.py).  When the LLM wants to transition state, it should simply use the TRANSITION_STATE action from backend/src/discovita/service/coach/models/action.py.  

The entirety of the coach logic is quite easy to describe:

1.  Current coach state + user message is passed to coach service. 
2.  Appropriate prompt is selected by the coach's prompt manager and sent to the LLM, with variables filled in using the current state values. 
3.  LLM returns a response object with a response message and actions. 
4.  Actions are carried out by an action executor.  One of these possible actions is to transition the state.  

        </note>
    </long-term-memory>
    <current-task> 
      In the coach's prompts, you will find that there are referenced,
      I believe in the "Identity Brainstorming" phase-specific prompts,
      a set of Identity Categories.  

      These Identity Categories may or may not also be in the codebase
      as an enum.  If they are not, I would like them to be. Moreover:

      when the coach creates an identity, I would like that identity
      to be formally and explicitly associated with an identity category.

      This means the following changes need to take place:

      1. The action backend/src/discovita/service/coach/models/action.py
      for creating identities needs a new parameter for identity
      category.
      2.  ANY prompts which invite the LLM to take this action need
      to describe the new parameter we are adding, including providing
      the list of acceptable enum values for identity categories.
      3.  The Identity object needs to be parsed by the LLM response
      parser.  The execution should fail fast if the identity category
      is missing or not one of the enum values.  It should be required
      in the identity constructor.
      4.  Global codebase search to find any usages that need to be updated.

      Let me know if I missed anything in this plan.

      Before you begin, write a doc 
      cline/coach_planning/v4_planning/explicit_categories.md
      which lists, using very short snippets labeled by their file,
      what changes you expect to make to the actual codebase.

      We made these changes, but there is now a server error when the 
      coach attempts to make amn identity.  Debug:
      <error>
      [backend] INFO:openai_client:Content: {
[backend]   "message": "Fantastic! Being a piano player and a programmer showcases your creativity and analytical skills. Let's start by creating identities that capture these passions and talents.",
[backend]   "actions": [
[backend]     {
[backend]       "type": "create_identity",
[backend]       "params": {
[backend]         "description": "Piano Player",
[backend]         "note": "Created this identity because the user identifies as a piano player, which highlights their musical talent and creative expression.",
[backend]         "category": "PASSIONS"
[backend]       }
[backend]     },
[backend]     {
[backend]       "type": "create_identity",
[backend]       "params": {
[backend]         "description": "Programmer",
[backend]         "note": "Created this identity because the user identifies as a programmer, showcasing their problem-solving skills and logical thinking.",
[backend]         "category": "PASSIONS"
[backend]       }
[backend]     }
[backend]   ]
[backend] }
[backend] 2025-03-31 14:13:36,614 - openai_client - INFO - Content: {
[backend]   "message": "Fantastic! Being a piano player and a programmer showcases your creativity and analytical skills. Let's start by creating identities that capture these passions and talents.",
[backend]   "actions": [
[backend]     {
[backend]       "type": "create_identity",
[backend]       "params": {
[backend]         "description": "Piano Player",
[backend]         "note": "Created this identity because the user identifies as a piano player, which highlights their musical talent and creative expression.",
[backend]         "category": "PASSIONS"
[backend]       }
[backend]     },
[backend]     {
[backend]       "type": "create_identity",
[backend]       "params": {
[backend]         "description": "Programmer",
[backend]         "note": "Created this identity because the user identifies as a programmer, showcasing their problem-solving skills and logical thinking.",
[backend]         "category": "PASSIONS"
[backend]       }
[backend]     }
[backend]   ]
[backend] }
[backend] INFO:openai_client:Model: gpt-4o-2024-08-06
[backend] 2025-03-31 14:13:36,614 - openai_client - INFO - Model: gpt-4o-2024-08-06
[backend] INFO:openai_client:Usage: CompletionUsage(completion_tokens=165, prompt_tokens=2021, total_tokens=2186, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))
[backend] 2025-03-31 14:13:36,614 - openai_client - INFO - Usage: CompletionUsage(completion_tokens=165, prompt_tokens=2021, total_tokens=2186, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))
[backend] INFO:openai_client:=== End Response ===
[backend] 2025-03-31 14:13:36,614 - openai_client - INFO - Content: {
[backend]   "message": "Fantastic! Being a piano player and a programmer showcases your creativity and analytical skills. Let's start by creating identities that capture these passions and talents.",
[backend]   "actions": [
[backend]     {
[backend]       "type": "create_identity",
[backend]       "params": {
[backend]         "description": "Piano Player",
[backend]         "note": "Created this identity because the user identifies as a piano player, which highlights their musical talent and creative expression.",
[backend]         "category": "PASSIONS"
[backend]       }
[backend]     },
[backend]     {
[backend]       "type": "create_identity",
[backend]       "params": {
[backend]         "description": "Programmer",
[backend]         "note": "Created this identity because the user identifies as a programmer, showcasing their problem-solving skills and logical thinking.",
[backend]         "category": "PASSIONS"
[backend]       }
[backend]     }
[backend]   ]
[backend] }
[backend] 2025-03-31 14:13:36,614 - openai_client - INFO - Model: gpt-4o-2024-08-06
[backend] 2025-03-31 14:13:36,614 - openai_client - INFO - Usage: CompletionUsage(completion_tokens=165, prompt_tokens=2021, total_tokens=2186, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))
[backend] 2025-03-31 14:13:36,614 - openai_client - INFO - === End Response ===
[backend] 2025-03-31 14:13:36,614 - openai_client - INFO - === End Response ===
[backend] INFO:     127.0.0.1:55857 - "POST /api/v1/coach/user_input HTTP/1.1" 500 Internal Server Error
[backend] ERROR:    Exception in ASGI application
[backend] Traceback (most recent call last):
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
[backend]     result = await app(  # type: ignore[func-returns-value]
[backend]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
[backend]     return await self.app(scope, receive, send)
[backend]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/fastapi/applications.py", line 1054, in __call__
[backend]     await super().__call__(scope, receive, send)
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/applications.py", line 112, in __call__
[backend]     await self.middleware_stack(scope, receive, send)
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/middleware/errors.py", line 187, in __call__
[backend]     raise exc
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/middleware/errors.py", line 165, in __call__
[backend]     await self.app(scope, receive, _send)
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/middleware/cors.py", line 93, in __call__
[backend]     await self.simple_response(scope, receive, send, request_headers=headers)
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/middleware/cors.py", line 144, in simple_response
[backend]     await self.app(scope, receive, send)
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py", line 62, in __call__
[backend]     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
[backend]     raise exc
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
[backend]     await app(scope, receive, sender)
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/routing.py", line 715, in __call__
[backend]     await self.middleware_stack(scope, receive, send)
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/routing.py", line 735, in app
[backend]     await route.handle(scope, receive, send)
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/routing.py", line 288, in handle
[backend]     await self.app(scope, receive, send)
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/routing.py", line 76, in app
[backend]     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
[backend]     raise exc
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
[backend]     await app(scope, receive, sender)
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/starlette/routing.py", line 73, in app
[backend]     response = await f(request)
[backend]                ^^^^^^^^^^^^^^^^
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/fastapi/routing.py", line 301, in app
[backend]     raw_response = await run_endpoint_function(
[backend]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/fastapi/routing.py", line 212, in run_endpoint_function
[backend]     return await dependant.call(**values)
[backend]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/src/discovita/api/routes/coach.py", line 16, in handle_user_input
[backend]     result = await service.process_message(request.message, request.coach_state)
[backend]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/src/discovita/service/coach/service.py", line 78, in process_message
[backend]     new_state = apply_actions(state, llm_response.actions)
[backend]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/src/discovita/service/coach/actions/handler.py", line 24, in apply_actions
[backend]     params = CreateIdentityParams(**action.params)
[backend]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[backend]   File "/Users/jakemirra/workspace/discovita/identity-generator/backend/venv/lib/python3.11/site-packages/pydantic/main.py", line 214, in __init__
[backend]     validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
[backend]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[backend] pydantic_core._pydantic_core.ValidationError: 1 validation error for CreateIdentityParams
[backend] category
[backend]   Input should be 'passions_and_talents', 'maker_of_money', 'keeper_of_money', 'spiritual', 'personal_appearance', 'physical_expression', 'familial_relations', 'romantic_relation' or 'doer_of_things' [type=enum, input_value='PASSIONS', input_type=str]
[backend]     For further information visit https://errors.pydantic.dev/2.10/v/enum
      </error>
    </current-task>
</cline-context>
